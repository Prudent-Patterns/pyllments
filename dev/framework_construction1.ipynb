{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import panel as pn\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import param\n",
    "\n",
    "from pyllments.base.model_base import Model\n",
    "from pyllments.payloads.message import MessagePayload\n",
    "\n",
    "class ChatInterfaceModel(Model):\n",
    "    # TODO: Implement batch interface for messages - populating message_list > iterating\n",
    "    message_list = param.List(instantiate=True)\n",
    "    persist = param.Boolean(default=False, instantiate=True)\n",
    "    new_message = param.ClassSelector(class_=MessagePayload)\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        super().__init__(**params)\n",
    "\n",
    "        self._create_watchers()\n",
    "\n",
    "    def _create_watchers(self):\n",
    "        self.param.watch(self._new_message_updated, 'new_message', precedence=10)\n",
    "\n",
    "    def _new_message_updated(self, event):\n",
    "        if self.new_message.model.mode == 'stream':\n",
    "            self.new_message.model.stream()\n",
    "        self.message_list.append(self.new_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "async_generator"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def generate_messages():\n",
    "    \"\"\"\n",
    "    Generator to yield messages from the message list.\n",
    "    \"\"\"\n",
    "    for message in [1,2,3]:\n",
    "        yield message\n",
    "\n",
    "type(generate_messages())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import param\n",
    "\n",
    "from pyllments.base.element_base import Element\n",
    "from pyllments.base.model_base import Model\n",
    "from pyllments.elements.chat_interface import ChatInterfaceModel\n",
    "from pyllments.payloads.message import MessagePayload\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "class ChatInterfaceElement(Element):\n",
    "    \"\"\"\n",
    "    Model:\n",
    "    - messages in the chat\n",
    "    - message input\n",
    "    Views:\n",
    "    - chat feed: \n",
    "    - chat input\n",
    "    - send button:\n",
    "    Ports:\n",
    "    - input:\n",
    "        - message_input: MessagePayload\n",
    "    - output port\n",
    "        - message_output: MessagePayload\n",
    "    \"\"\"\n",
    "    model = param.ClassSelector(\n",
    "        class_=Model,\n",
    "        is_instance=True\n",
    "    )\n",
    "    model_params = param.Dict(default={})\n",
    "\n",
    "    chatfeed_view = param.ClassSelector(class_=pn.Column, is_instance=True)\n",
    "    chat_input_view = param.ClassSelector(class_=pn.chat.ChatAreaInput, is_instance=True)\n",
    "    send_button_view = param.ClassSelector(class_=pn.widgets.Button, is_instance=True)\n",
    "\n",
    "    def __init__(self, persist=False, **params):\n",
    "        super().__init__(**params)\n",
    "        self.model = ChatInterfaceModel(**self.model_params)\n",
    "        \n",
    "        self.message_output_setup()\n",
    "        self.message_input_setup()\n",
    "\n",
    "    def message_output_setup(self):\n",
    "        \"\"\"Sets up the output message port\"\"\"\n",
    "        def pack(new_message: MessagePayload) -> MessagePayload:\n",
    "            return new_message\n",
    "\n",
    "        self.ports.add_output(\n",
    "            name='message_output',\n",
    "            pack_payload_callback=pack)\n",
    "    \n",
    "    def message_input_setup(self):\n",
    "        \"\"\"Sets up the input message port\"\"\"\n",
    "        def unpack(payload: MessagePayload):\n",
    "            self.model.new_message = payload\n",
    "        \n",
    "        self.ports.add_input(\n",
    "            name='message_input',\n",
    "            unpack_payload_callback=unpack)\n",
    "\n",
    "    def create_chatfeed_view(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates and returns a new instance of the chatfeed whichi\n",
    "        contains the visual components of the message payloads.\n",
    "        \"\"\"\n",
    "        if self._view_exists(self.chatfeed_view):\n",
    "            return self.chatfeed_view\n",
    "        # When first loaded\n",
    "        self.chatfeed_view = pn.Column(**kwargs)\n",
    "        message_views = [\n",
    "            message.create_message_view() \n",
    "            for message in self.model.message_list\n",
    "        ]\n",
    "        self.chatfeed_view.extend(message_views)\n",
    "\n",
    "        def _update_chatfeed(event):\n",
    "            self.chatfeed_view.append(event.new.create_message_view())\n",
    "        # This watcher should be called before the payload starts streaming.\n",
    "        self.model.param.watch(_update_chatfeed, 'new_message', precedence=0)\n",
    "        return self.chatfeed_view\n",
    "\n",
    "\n",
    "    def create_chat_input_view(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates and returns a new instance of ChatAreaInput view.\n",
    "        \"\"\"\n",
    "        if self._view_exists(self.chat_input_view):\n",
    "            return self.chat_input_view\n",
    "\n",
    "        self.chat_input_view = pn.chat.ChatAreaInput(\n",
    "            placeholder='Enter your message',\n",
    "            **kwargs)\n",
    "        self.chat_input_view.param.watch(self._on_send, 'value')\n",
    "        return self.chat_input_view\n",
    "    \n",
    "\n",
    "    def create_send_button_view(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates and returns a new instance of Button view for sending messages.\n",
    "        \"\"\"\n",
    "        if self._view_exists(self.send_button_view):\n",
    "            return self.send_button_view\n",
    "\n",
    "        self.send_button_view = pn.widgets.Button(name='send', **kwargs)\n",
    "        self.send_button_view.on_click(self._on_send)\n",
    "\n",
    "        return self.send_button_view\n",
    "    \n",
    "    @Element.port_stage_emit_if_exists('message_output', 'new_message')\n",
    "    def _on_send(self, event):\n",
    "        \"\"\"\n",
    "        Handles the send button event by appending the user's message to the chat model,\n",
    "        clearing the input field, and updating the chat feed view.\n",
    "        \"\"\"\n",
    "        \n",
    "        if event.obj is self.send_button_view: # When send button is clicked\n",
    "            if self.chat_input_view:\n",
    "                input_text = self.chat_input_view.value_input\n",
    "                self.chat_input_view.value_input = ''\n",
    "                new_message = MessagePayload(\n",
    "                    message=HumanMessage(input_text),\n",
    "                    mode='atomic')\n",
    "            self.model.new_message = new_message\n",
    "            \n",
    "        elif event.obj is self.chat_input_view: # When value changes on 'enter'\n",
    "            input_text = self.chat_input_view.value\n",
    "            new_message = MessagePayload(\n",
    "                message=HumanMessage(input_text),\n",
    "                mode='atomic')\n",
    "            self.model.new_message = new_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panel.theme import Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb542eef5c9490fb031bc3d95e91aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'e8635e5e-4579-45cc-b501-e78ea89219a7': {'version…"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stylesheet = \"\"\"\n",
    ".bk-input-group {\n",
    "    font-family: '';\n",
    "}\n",
    "\"\"\"\n",
    "pn.widgets.TextInput(placeholder='Enter your message', stylesheets=[stylesheet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_interface_element = ChatInterfaceElement()\n",
    "# chat_input_view = chat_interface_element.create_chat_input_view()\n",
    "# # chat_input_view.value = 'hello'\n",
    "\n",
    "# send_button_view = chat_interface_element.create_send_button_view()\n",
    "# pn.Row(chat_input_view, send_button_view).servable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import param\n",
    "\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "from pyllments.base.model_base import Model\n",
    "\n",
    "class LLMChatModel(Model):\n",
    "    chat_model = param.ClassSelector(class_=BaseLanguageModel, doc=\"\"\"\n",
    "        Instance of active chat model.\"\"\")\n",
    "\n",
    "    model_args = param.Dict(default={}, doc=\"\"\"\n",
    "        Takes a dictionary of arguments to pass to expose and send to the\n",
    "        model class. If you set a None value as the key, the argument will be exposed,\n",
    "        with the default value set. Passing a list is the same as passing a dict\n",
    "        with all values set to None.\"\"\") # TODO Allow nested dict for model_name: model_args format\n",
    "    \n",
    "    provider_name = param.String(default='openai', doc='Provider of the model')\n",
    "    model_name = param.String(default='gpt-3.5-turbo', doc='Name of the model')\n",
    "    incoming_messages = param.List(item_type=BaseMessage)\n",
    "    output_mode = param.Selector(\n",
    "        objects=['atomic', 'stream'],\n",
    "        default='stream',\n",
    "        )\n",
    "    # new_message = param.ClassSelector(class_=AIMessage)\n",
    "    outgoing_message = param.Parameter(doc=\"\"\"\n",
    "        AIMessage or Stream, depending on output_mode\"\"\")\n",
    "\n",
    "    def __init__(self, **params):\n",
    "        super().__init__(**params)\n",
    "        self._initialize_provider()\n",
    "        self._initialize_model()\n",
    "        # self._set_params()\n",
    "    #     self._create_watchers()\n",
    "\n",
    "    # def _create_watchers(self):\n",
    "    #     self.param.watch(\n",
    "    #         self._new_outgoing_message,\n",
    "    #         'outgoing_message',\n",
    "    #         onlychanged=False\n",
    "    #         )\n",
    "    # def _set_params(self):\n",
    "    #     \"\"\"Sets specified model_args as params of the object\"\"\"\n",
    "    #     if self.model_args:\n",
    "    #         for arg, val in self.model_args.items():\n",
    "    #             if arg in self.model_class.__fields__:\n",
    "    #                 if val is None:\n",
    "    #                     default = self.model_class.__fields__[arg].default\n",
    "    #                     self.model_args[arg] = default\n",
    "    #                     self.param.add_parameter(arg, param.Parameter(default, per_instance=True))\n",
    "    #                 else:\n",
    "    #                     self.param.add_parameter(arg, param.Parameter(val, per_instance=True))\n",
    "    #                 # self.model_args_list.append(arg)\n",
    "    #             else:\n",
    "    #                 raise ValueError(f\"'{arg}' is missing from the model class's signature\")\n",
    "    #             self.param.watch(self._create_model, [*self.model_args.keys()])\n",
    "\n",
    "\n",
    "    # def _create_model(self, event=None):\n",
    "    #     \"\"\"Creates the model instance on init and when any of the parameters change\"\"\"\n",
    "    #     arg_vals = {arg: self.param.values()[arg] for arg in self.model_args.keys()}\n",
    "    #     self.model = self.model_class(**arg_vals)\n",
    "    \n",
    "    def _initialize_provider(self):\n",
    "        \"\"\"Initializes the provider\"\"\"\n",
    "        match self.provider_name:\n",
    "            case 'openai':\n",
    "                from langchain_openai import ChatOpenAI\n",
    "                self.provider = ChatOpenAI\n",
    "            case 'anthropic':\n",
    "                from langchain_anthropic import ChatAnthropic\n",
    "                self.provider = ChatAnthropic\n",
    "            case 'groq':\n",
    "                from langchain_groq import ChatGroq\n",
    "                self.provider = ChatGroq\n",
    "            case 'mistral':\n",
    "                from langchain_mistralai import ChatMistralAI\n",
    "                self.provider = ChatMistralAI\n",
    "            case 'google':\n",
    "                from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "                self.provider = ChatGoogleGenerativeAI\n",
    "            case _:\n",
    "                raise ValueError(f\"Provider name '{self.provider_name}' is not valid\")\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initializes the model\"\"\"\n",
    "        self.chat_model = self.provider(model_name=self.model_name, **self.model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import AsyncGenerator, Generator\n",
    "\n",
    "import param\n",
    "import panel as pn\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from pyllments.base.element_base import Element\n",
    "from pyllments.payloads.message import MessagePayload\n",
    "# from pyllments.elements.llm_chat import LLMChatModel\n",
    "\n",
    "class LLMChatElement(Element):\n",
    "    model = param.ClassSelector(class_=Model)\n",
    "    model_params = param.Dict(default={})\n",
    "  \n",
    "    def __init__(self, **params):\n",
    "        super().__init__(**params)\n",
    "        self.model = LLMChatModel(**self.model_params)\n",
    "\n",
    "        self._message_output_setup()\n",
    "        self._messages_input_setup()\n",
    "\n",
    "        self._create_watchers()\n",
    "        \n",
    "    def _message_output_setup(self):\n",
    "        if self.model.output_mode == 'stream':\n",
    "            def pack(outgoing_message: AsyncGenerator | Generator) -> MessagePayload:\n",
    "                payload = MessagePayload(\n",
    "                    message_type='ai',\n",
    "                    message_stream=outgoing_message,\n",
    "                    mode='stream'\n",
    "                )\n",
    "                return payload\n",
    "        elif self.model.output_mode == 'atomic':\n",
    "            def pack(outgoing_message: AIMessage) -> MessagePayload:\n",
    "                payload = MessagePayload(\n",
    "                    message=outgoing_message,\n",
    "                    mode='atomic'\n",
    "                )\n",
    "                return payload\n",
    "            \n",
    "        self.ports.add_output(name='message_output', pack_payload_callback=pack)\n",
    "\n",
    "    def _messages_input_setup(self):\n",
    "        def unpack(payload: MessagePayload):\n",
    "            if payload.model.mode == 'atomic':\n",
    "                if self.model.output_mode == 'atomic':\n",
    "                    self.model.outgoing_message = self.model.chat_model.invoke(\n",
    "                        [payload.model.message]\n",
    "                    )\n",
    "                elif self.model.output_mode == 'stream':\n",
    "                    self.model.outgoing_message = self.model.chat_model.stream(\n",
    "                        [payload.model.message]\n",
    "                    )\n",
    "            elif payload.model.mode == 'batch':\n",
    "                if self.model.output_mode == 'atomic':\n",
    "                    self.model.outgoing_message = self.model.chat_model.invoke(\n",
    "                        payload.model.messages_batch\n",
    "                    )\n",
    "                elif self.model.output_mode == 'stream':\n",
    "                    self.model.outgoing_message = self.model.chat_model.stream(\n",
    "                        payload.model.messages_batch\n",
    "                    )\n",
    "        self.ports.add_input(name='messages_input', unpack_payload_callback=unpack)\n",
    "\n",
    "    def _create_watchers(self):\n",
    "        self.model.param.watch(self._outgoing_message_updated, 'outgoing_message')\n",
    "    \n",
    "    @Element.port_stage_emit_if_exists('message_output', 'outgoing_message')\n",
    "    def _outgoing_message_updated(self, event):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # def create_temperature_view(self, **kwargs):\n",
    "    #     self.temperature_view = pn.widgets.FloatSlider(**kwargs)\n",
    "    #     return self.temperature_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-779fc32a-3f73-454d-b113-7b8e095cec02-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat_interface = ChatInterfaceElement()\n",
    "\n",
    "# llm_chat = LLMChatElement(model_params={'output_mode': 'atomic'})\n",
    "\n",
    "# chat_interface.ports.output['message_output'] > llm_chat.ports.input['messages_input']\n",
    "# # chat_interface.model.new_message = MessagePayload(\n",
    "# #     message=HumanMessage('hello'),\n",
    "# #     mode='atomic'\n",
    "# # )\n",
    "# chat_interface.ports.output['message_output'].stage_emit(\n",
    "#     message_payload=MessagePayload(\n",
    "#         message=HumanMessage('hello'),\n",
    "#         mode='atomic'\n",
    "# ))\n",
    "\n",
    "# llm_chat.model.outgoing_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BaseChatModel.stream at 0x7fc402fa18b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat_interface = ChatInterfaceElement()\n",
    "\n",
    "# llm_chat = LLMChatElement(model_params={'output_mode': 'stream'})\n",
    "\n",
    "# chat_interface.ports.output['message_output'] > llm_chat.ports.input['messages_input']\n",
    "# # chat_interface.model.new_message = MessagePayload(\n",
    "# #     message=HumanMessage('hello'),\n",
    "# #     mode='atomic'\n",
    "# # )\n",
    "# chat_interface.ports.output['message_output'].stage_emit(\n",
    "#     message_payload=MessagePayload(\n",
    "#         message=HumanMessage('hello'),\n",
    "#         mode='atomic'\n",
    "# ))\n",
    "\n",
    "# llm_chat.model.outgoing_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = ChatInterfaceElement()\n",
    "\n",
    "llm_chat = LLMChatElement(model_params={'output_mode': 'atomic'})\n",
    "\n",
    "chat_interface.ports.output['message_output'] > llm_chat.ports.input['messages_input']\n",
    "llm_chat.ports.output['message_output'] > chat_interface.ports.input['message_input']\n",
    "\n",
    "\n",
    "# chat_interface.ports.output['message_output'].stage_emit(\n",
    "#     message_payload=MessagePayload(\n",
    "#         message=HumanMessage('hello'),\n",
    "#         mode='atomic'\n",
    "# ))\n",
    "\n",
    "# llm_chat.model.outgoing_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from param import Parameterized, Parameter\n",
    "\n",
    "class SharedParameterClass(Parameterized):\n",
    "    shared_param = Parameter(per_instance=False)\n",
    "    instance_param = Parameter(per_instance=True)\n",
    "\n",
    "# Create two instances of the class\n",
    "instance1 = SharedParameterClass()\n",
    "instance2 = SharedParameterClass()\n",
    "\n",
    "# Verify that shared_param is shared between instances\n",
    "# instance1.shared_param = 3\n",
    "# print(instance2.shared_param)  # This should print 3\n",
    "\n",
    "# # Verify that instance_param is not shared between instances\n",
    "# instance1.instance_param = 4\n",
    "# print(instance2.instance_param)  # This should print 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if []:\n",
    "    print(\"we're in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['something', 'kwargs']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "import param\n",
    "\n",
    "class Test(param.Parameterized):\n",
    "    test = param.String()\n",
    "\n",
    "    def create_test_view(self, something=None, **kwargs):\n",
    "        return pn.widgets.TextInput(value=self.test)\n",
    "\n",
    "test = Test()\n",
    "list(inspect.signature(test.create_test_view).parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:param.Markdown00126: Displaying Panel objects in the notebook requires the panel extension to be loaded. Ensure you run pn.extension() before displaying objects in the notebook.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Markdown(str, stylesheets=[{}])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import panel as pn\n",
    "pn.pane.Markdown('something', stylesheets=[{}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a124fceb917640ab94a12342ac5dc79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'5af3ffae-6180-4873-a7f7-0e623d00d8b3': {'version…"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatfeed_view = chat_interface.create_chatfeed_view(height=300, auto_scroll_limit=1, width=500)\n",
    "send_button_view = chat_interface.create_send_button_view()\n",
    "chat_input_view = chat_interface.create_chat_input_view()\n",
    "\n",
    "pn.Column(\n",
    "    chatfeed_view,\n",
    "    pn.Row(chat_input_view, send_button_view)\n",
    ").servable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panel.theme import Design\n",
    "from panel.base import Da\n",
    "\n",
    "class pyllments_default(Design):\n",
    "    \n",
    "    modifiers = {}\n",
    "\n",
    "    _resources = {\n",
    "        'css': {\n",
    "            'material': \"https://cdnjs.cloudflare.com/ajax/libs/material-components-web/14.0.0/material-components-web.css\"\n",
    "        },\n",
    "        'fonts': {\n",
    "            'ubuntu': \"https://fonts.googleapis.com/css2?family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap\"\n",
    "        },\n",
    "        'js': {\n",
    "            'material': \"https://cdnjs.cloudflare.com/ajax/libs/material-components-web/14.0.0/material-components-web.min.js\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    _themes = {\n",
    "        'dark': MaterialDarkTheme\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyllments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
