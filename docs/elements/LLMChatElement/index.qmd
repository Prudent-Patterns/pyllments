---
title: "LLMChatElement"
lightbox: true
---

The `LLMChatElement` is a core component for integrating Large Language Models (LLMs) into chat applications. It handles the communication with LLM providers and generates responses based on input messages. Its interface is meant to receive `MessagePayloads` and respond with a `MessagePayload` in turn.

## Instantiation

**Arguments:**

`model_name`: str = 'gpt-4o-mini'<br>
<span class="tab">The name of the model to use for the LLM.</span>
`model_args`: dict = {}<br>
<span class="tab">Additional arguments to pass to the model.</span>
`base_url`: str = None<br>
<span class="tab">The base URL for the model. Optional, to be used with custom endpoints.</span>
`output_mode`: Literal['atomic', 'stream'] = 'stream'<br>
<span class="tab">Whether to stream the response or return it all at once.</span>

### Input Ports

| Port Name            | Payload Type                                          | Behavior                                                                 |
|----------------------|-------------------------------------------------------|---------------------------------------------------------------------------|
| messages_emit_input  | Union[MessagePayload, List[Union[MessagePayload, ToolsResponsePayload]]] | Processes incoming messages or lists of messages/tool responses to generate an LLM response which is emitted from the `message_output` port. |

: {.hover}

### Output Ports

| Port Name            | Payload Type          | Behavior                                                        |
|----------------------|-----------------------|-----------------------------------------------------------------|
| message_output       | MessagePayload        | Emits a `MessagePayload` containing the LLM's response to the next element. |

: {.hover}

### Views
{{< var views.styling >}}

| View Name            | Description                                                                 | Image                                      |
|----------------------|-----------------------------------------------------------------------------|--------------------------------------------|
| model_selector_view  | Allows selection of LLM provider and model for generating responses.<br>**Args:**<br>`models: list or dict, optional`<br><span class="tab">A list or dict of models. If a dict, the keys are used as display names.</span><br>`show_provider_selector: bool, default True`<br><span class="tab">Whether to include a provider selection widget.</span><br>`provider: str, default 'OpenAI'`<br><span class="tab">The default provider value.</span><br>`model: str or dict, default 'gpt-4o-mini'`<br><span class="tab">The default model (this update assumes it is a string to be pre-selected).</span><br>`orientation: {'vertical', 'horizontal'}, default 'horizontal'`<br><span class="tab">Layout orientation for the provider and model selectors.</span><br>`model_selector_width: int = None`<br>`provider_selector_width: int = None`<br>`selector_css: list[str] = []`<br>`height: int = 57` | With provider:<br>![ModelSelectorView](model_selector_model.jpg){.lightbox}<br>Without provider:<br>![ModelSelectorView](model_selector_provider_model.jpg){.lightbox} |



