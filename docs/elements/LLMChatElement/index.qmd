---
title: "LLMChatElement"
lightbox: true
---

The `LLMChatElement` is a core component for integrating Large Language Models (LLMs) into chat applications. It handles the communication with LLM providers and generates responses based on input messages. Its interface is meant to receive `list[MessagePayload]` and respond with a `MessagePayload` in turn.

## Instantiation

##### LiteLLM

Under the hood, this element uses [LiteLLM](https://docs.litellm.ai/docs/) and the standardized [model names](https://models.litellm.ai) used by APIs of various providers.
When this Element is instantiated, unless a `env_path` is provided, it will look in the working directory for an `.env` file.

##### APIs

We rely on the standard API key names such as `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `XAI_API_KEY`, et al to be present in the `.env` file depending on the model you wish to use. For more on authentication, see the [docs here](https://docs.litellm.ai/docs/set_keys).

##### OpenRouter

If you wish to try out many models with a single API key, we recommend you use [OpenRouter](https://docs.litellm.ai/docs/providers/openrouter), which is supported by LiteLLM. 

##### Local
Alternatively, you can also run local models(ollama, VLLM, etc), where you need to include the name and the `base_url`. See more on that [here](https://docs.litellm.ai/docs/providers/ollama).

**Arguments:**

`model_name`: str = 'gpt-4o-mini'<br>
<span class="tab">The name of the model to use for the LLM.</span>
`model_args`: dict = {}<br>
<span class="tab">Additional arguments to pass to the model.</span>
`base_url`: str = None<br>
<span class="tab">The base URL for the model. Optional, to be used with custom endpoints.</span>
`output_mode`: Literal['atomic', 'stream'] = 'stream'<br>
<span class="tab">Whether to return the message containing a streaming callback or an atomic one</span>
`env_path`: str = None<br>
<span class="tab">Path to the .env file to load. If not provided, the .env file in the current working directory will be used.</span>

### Input Ports

| Port Name            | Payload Type                                          | Behavior                                                                 |
|----------------------|-------------------------------------------------------|---------------------------------------------------------------------------|
| messages_emit_input  | Union[MessagePayload, List[Union[MessagePayload, ToolsResponsePayload]]] | Processes incoming messages or lists of messages/tool responses to generate an LLM response which is emitted from the `message_output` port. |

: {.hover}

### Output Ports

| Port Name            | Payload Type          | Behavior                                                        |
|----------------------|-----------------------|-----------------------------------------------------------------|
| message_output       | MessagePayload        | Emits a `MessagePayload` containing the LLM's response to the next element. |

: {.hover}

### Views
{{< var views.styling >}}

| View Name            | Description                                                                 | Image                                      |
|----------------------|-----------------------------------------------------------------------------|--------------------------------------------|
| model_selector_view  | Allows selection of LLM provider and model for generating responses.<br>**Args:**<br>`models: list or dict, optional`<br><span class="tab">A list or dict of models. If a dict, the keys are used as display names.</span><br>`show_provider_selector: bool, default True`<br><span class="tab">Whether to include a provider selection widget.</span><br>`provider: str, default 'OpenAI'`<br><span class="tab">The default provider value.</span><br>`model: str or dict, default 'gpt-4o-mini'`<br><span class="tab">The default model (this update assumes it is a string to be pre-selected).</span><br>`orientation: {'vertical', 'horizontal'}, default 'horizontal'`<br><span class="tab">Layout orientation for the provider and model selectors.</span><br>`model_selector_width: int = None`<br>`provider_selector_width: int = None`<br>`selector_css: list[str] = []`<br>`height: int = 57` | With provider:<br>![ModelSelectorView](model_selector_model.jpg){.lightbox}<br>Without provider:<br>![ModelSelectorView](model_selector_provider_model.jpg){.lightbox} |



